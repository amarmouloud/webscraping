{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1195eab-5a13-4d2b-a0af-d4105cc45e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 22:38:30 [numexpr.utils] INFO: NumExpr defaulting to 4 threads.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "805ce4a2-cc1f-466a-890a-d850dbdaa5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\"https://nominatim.openstreetmap.org/search?city=Paris&format=json\")\n",
    "\n",
    "# now let's look at what's in this variable\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba3406c-f287-44a3-a9e6-77797fb8a1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'place_id': 297417241,\n",
       " 'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright',\n",
       " 'osm_type': 'relation',\n",
       " 'osm_id': 7444,\n",
       " 'boundingbox': ['48.8155755', '48.902156', '2.224122', '2.4697602'],\n",
       " 'lat': '48.8588897',\n",
       " 'lon': '2.3200410217200766',\n",
       " 'display_name': 'Paris, Île-de-France, France métropolitaine, France',\n",
       " 'class': 'boundary',\n",
       " 'type': 'administrative',\n",
       " 'importance': 0.9417101715588673,\n",
       " 'icon': 'https://nominatim.openstreetmap.org/ui/mapicons/poi_boundary_administrative.p.20.png'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premier_resultat=r.json()[0]\n",
    "premier_resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5924259-89b4-4aee-a6f7-a8d56d47af6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'48.8588897'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premier_resultat.get(\"lat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801b1f5f-1adf-4142-9715-16ad308e7821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3200410217200766'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premier_resultat.get(\"lon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2a5cc33-6daa-45b6-b356-828e9bec1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\"Mont Saint Michel\",\n",
    "\"St Malo\",\n",
    "\"Bayeux\",\n",
    "\"Le Havre\",\n",
    "\"Rouen\",\n",
    "\"Paris\",\n",
    "\"Amiens\",\n",
    "\"Lille\",\n",
    "\"Strasbourg\",\n",
    "\"Chateau du Haut Koenigsbourg\",\n",
    "\"Colmar\",\n",
    "\"Eguisheim\",\n",
    "\"Besancon\",\n",
    "\"Dijon\",\n",
    "\"Annecy\",\n",
    "\"Grenoble\",\n",
    "\"Lyon\",\n",
    "\"Gorges du Verdon\",\n",
    "\"Bormes les Mimosas\",\n",
    "\"Cassis\",\n",
    "\"Marseille\",\n",
    "\"Aix en Provence\",\n",
    "\"Avignon\",\n",
    "\"Uzes\",\n",
    "\"Nimes\",\n",
    "\"Aigues Mortes\",\n",
    "\"Saintes Maries de la mer\",\n",
    "\"Collioure\",\n",
    "\"Carcassonne\",\n",
    "\"Ariege\",\n",
    "\"Toulouse\",\n",
    "\"Montauban\",\n",
    "\"Biarritz\",\n",
    "\"Bayonne\",\n",
    "\"La Rochelle\"]\n",
    "\n",
    "list_latitude = []\n",
    "list_longitude = []\n",
    "\n",
    "for i in cities :\n",
    "    r = requests.get(f\"https://nominatim.openstreetmap.org/search?q={i}&format=json\")\n",
    "    premier_resultat=r.json()[0]\n",
    "    lat= premier_resultat.get(\"lat\")\n",
    "    long =premier_resultat.get(\"lon\")\n",
    "    list_latitude.append(lat)\n",
    "    list_longitude.append(long)\n",
    "    \n",
    "    r = requests.get(f\"https://api.openweathermap.org/data/2.5/onecall?lat={lat}&lon={long}&exclude=current,minutely,hourly&appid=d50a5226032b0065670bde9f54300a91\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6849f533-54df-4771-b553-6457a2b04d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data['city'] = cities\n",
    "data['lat'] = list_latitude\n",
    "data['long'] = list_longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8302e5f1-22ca-4ca1-b700-ecbee7b7f6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mont Saint Michel</td>\n",
       "      <td>48.6359541</td>\n",
       "      <td>-1.511459954959514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>St Malo</td>\n",
       "      <td>48.649518</td>\n",
       "      <td>-2.0260409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bayeux</td>\n",
       "      <td>49.2764624</td>\n",
       "      <td>-0.7024738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Le Havre</td>\n",
       "      <td>49.4938975</td>\n",
       "      <td>0.1079732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rouen</td>\n",
       "      <td>49.4404591</td>\n",
       "      <td>1.0939658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                city         lat                long\n",
       "0  Mont Saint Michel  48.6359541  -1.511459954959514\n",
       "1            St Malo   48.649518          -2.0260409\n",
       "2             Bayeux  49.2764624          -0.7024738\n",
       "3           Le Havre  49.4938975           0.1079732\n",
       "4              Rouen  49.4404591           1.0939658"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b82f246-f71e-4289-ab1d-c45e512f70e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = 48.649518\n",
    "lon = -2.0260409"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b025165f-499b-4d90-b1c9-88b82a56b765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [401]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(f\"https://api.openweathermap.org/data/2.5/onecall?lat={lat}&lon={lon}&exclude=current,minutely,hourly&appid=d50a5226032b0065670bde9f54300a91\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7472750-7ba1-43cc-be2b-3db1fbb1e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Scrapy in /opt/conda/lib/python3.9/site-packages (2.7.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (2.0.6)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from Scrapy) (59.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from Scrapy) (21.3)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (1.7.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (0.2.1)\n",
      "Requirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (36.0.1)\n",
      "Requirement already satisfied: tldextract in /opt/conda/lib/python3.9/site-packages (from Scrapy) (3.4.0)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (21.1.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (0.7.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (2.0.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (22.0.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (1.0.6)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (1.6.2)\n",
      "Requirement already satisfied: lxml>=4.3.0 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (4.9.1)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (5.5.1)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (1.2.0)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in /opt/conda/lib/python3.9/site-packages (from Scrapy) (22.10.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.9/site-packages (from cryptography>=3.3->Scrapy) (1.15.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /opt/conda/lib/python3.9/site-packages (from itemloaders>=1.0.1->Scrapy) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from protego>=0.1.15->Scrapy) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.9/site-packages (from service-identity>=18.1.0->Scrapy) (0.2.8)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.9/site-packages (from service-identity>=18.1.0->Scrapy) (21.4.0)\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.9/site-packages (from service-identity>=18.1.0->Scrapy) (0.4.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.9/site-packages (from Twisted>=18.9.0->Scrapy) (4.0.1)\n",
      "Requirement already satisfied: Automat>=0.8.0 in /opt/conda/lib/python3.9/site-packages (from Twisted>=18.9.0->Scrapy) (22.10.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in /opt/conda/lib/python3.9/site-packages (from Twisted>=18.9.0->Scrapy) (22.10.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/conda/lib/python3.9/site-packages (from Twisted>=18.9.0->Scrapy) (21.0.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/conda/lib/python3.9/site-packages (from Twisted>=18.9.0->Scrapy) (15.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->Scrapy) (3.0.7)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from tldextract->Scrapy) (3.8.0)\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/conda/lib/python3.9/site-packages (from tldextract->Scrapy) (2.27.1)\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/conda/lib/python3.9/site-packages (from tldextract->Scrapy) (1.5.1)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.9/site-packages (from tldextract->Scrapy) (3.3)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.3->Scrapy) (2.21)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.1.0->tldextract->Scrapy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.1.0->tldextract->Scrapy) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.1.0->tldextract->Scrapy) (2.0.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3847a69-79d5-40a8-8153-236c955cf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b944c30-e5e9-4337-b930-f47685b2232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomQuoteSpider(scrapy.Spider):\n",
    "    name = \"Booking\"\n",
    "    \n",
    "    Start_urls = [\n",
    "        'https://www.booking.com/hotel'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        données = response.css('div.d20f4628d0').get()\n",
    "        return{\n",
    "            'nom-hotel' :données.css('div.fcab3ed991.a23c043802::text').get,\n",
    "            'coordonnées': données.css('span.4bd0794db.b4273d69aa::text').get(),\n",
    "            'notes' : données.css('div.\"b5cd09854e.d10a6220b4::text').get(),\n",
    "            'description' : données.css('div.d8eab2cf7f::text').get()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da951e41-4c90-47a3-b312-3c0b4ac38b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 22:10:19 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: scrapybot)\n",
      "2022-11-16 22:10:19 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.0.1, Twisted 22.10.0, Python 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 36.0.1, Platform Linux-5.10.133+-x86_64-with-glibc2.31\n",
      "2022-11-16 22:10:19 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2022-11-16 22:10:19 [py.warnings] WARNING: /opt/conda/lib/python3.9/site-packages/scrapy/utils/request.py:231: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2022-11-16 22:10:19 [scrapy.extensions.telnet] INFO: Telnet Password: 4892a2bd9275fe53\n",
      "2022-11-16 22:10:19 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-11-16 22:10:19 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-11-16 22:10:19 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-11-16 22:10:19 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-11-16 22:10:19 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-11-16 22:10:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-11-16 22:10:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-11-16 22:10:19 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-11-16 22:10:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'elapsed_time_seconds': 0.007434,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 11, 16, 22, 10, 19, 293340),\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 101527552,\n",
      " 'memusage/startup': 101527552,\n",
      " 'start_time': datetime.datetime(2022, 11, 16, 22, 10, 19, 285906)}\n",
      "2022-11-16 22:10:19 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"1_Booking.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(RandomQuoteSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "012c141f-4d8e-43fd-850f-2712fa58a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "875144da-8a30-491a-a57d-ea5b53852a9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'yield' outside function (1483405390.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [16]\u001b[0;36m\u001b[0m\n\u001b[0;31m    yield response.follow(next_page, callback=self.parse)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'yield' outside function\n"
     ]
    }
   ],
   "source": [
    "class QuotemultipleSpider(scrapy.Spider):\n",
    "    name = \"Bookingmultiplespages\"\n",
    "    \n",
    "    Start_urls = [\n",
    "        'https://www.booking.com/hotel/page/1/'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for données in response.css('div.d20f4628d0'):\n",
    "            yield {\n",
    "            'nom-hotel' :données.css('div.fcab3ed991.a23c043802::text').get,\n",
    "            'coordonnées': données.css('span.4bd0794db.b4273d69aa::text').get(),\n",
    "            'notes' : données.css('div.\"b5cd09854e.d10a6220b4::text').get(),\n",
    "            'description' : données.css('div.d8eab2cf7f::text').get()\n",
    "        }\n",
    "        \n",
    "    try :\n",
    "        next_page = response.css('button.fc63351294.f9c5690c58 li').attrib[\"f32a99c8d1\"]\n",
    "    except KeyError:\n",
    "        logging.info('No next page. Terminating crawling process.')\n",
    "    else:\n",
    "        \n",
    "        yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c96746-ce3b-4b0c-8f82-741d82dbaede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"3_Booking.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename : {\"format\": \"json\"},\n",
    "    },\n",
    "    \"AUTOTHROTTLE_ENABLED\" : True\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(RandomQuoteSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
